{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l0Xf3m6AGKjF"
      },
      "outputs": [],
      "source": [
        "!pip install -q unsloth transformers accelerate peft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "1eb630edc237401595c86cd685027699",
            "1a7534461fd94b9f8ec5722df163cf58",
            "2d60c292d7d44f25a1862e85dd637cff",
            "786555df799b4fa0b563781d1e6103bb",
            "75ab2321f8434fda8debb1bf9777663c",
            "d3d012f9279b4d5a99bd2a7be67f4453",
            "8cc9cabb6c7a4b74b6e5cbbe1f034b41",
            "8847fbc5ade84e1bb0085df55602e5db",
            "1c612521a0c34204b4bfd73856ad4c53",
            "40603c456cdb4c49bc4d007bdcaccd06",
            "f7c8938744d14b9cae922e168244bfd3"
          ]
        },
        "id": "zW2l2AH8N1-_",
        "outputId": "35ae292c-dccd-4463-fd1f-428a0de2c066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Loading model from HuggingFace...\n",
            "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1eb630edc237401595c86cd685027699",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# Load Model from HuggingFace\n",
        "# -----------------------------\n",
        "HF_MODEL_ID = \"anonym-author/llama3-8b-instruct-arabic\"\n",
        "\n",
        "print(\"Loading model from HuggingFace...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=HF_MODEL_ID,\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,  # Use 4-bit to save memory in Colab\n",
        ")\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TU-y8XPaNVqp"
      },
      "outputs": [],
      "source": [
        "def isTrigger(text: str):\n",
        "    \"\"\"\n",
        "    Classify patient statement as trigger (1) or non-trigger (0).\n",
        "\n",
        "    Args:\n",
        "        text: Patient statement in English or Arabic\n",
        "\n",
        "    Returns:\n",
        "        dict with 'label' and 'confidence'\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"Classify the following patient statement as 0 (Not Triggered) or 1 (Triggered).\\n\"\n",
        "        \"Respond with exactly one character: 0 or 1.\\n\\n\"\n",
        "        \"### Statement:\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "        \"### Response:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Get logits instead of generating\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)\n",
        "        logits = outputs.logits[0, -1, :]  # Logits for the next token\n",
        "\n",
        "        # Get token IDs for \"0\" and \"1\"\n",
        "        token_0 = tokenizer.encode(\"0\", add_special_tokens=False)[0]\n",
        "        token_1 = tokenizer.encode(\"1\", add_special_tokens=False)[0]\n",
        "\n",
        "        # Get probabilities for these tokens\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        prob_0 = probs[token_0].item()\n",
        "        prob_1 = probs[token_1].item()\n",
        "\n",
        "        # Predict based on which has higher probability\n",
        "        if prob_1 > prob_0:\n",
        "            predicted_label = 1\n",
        "            confidence = prob_1\n",
        "        else:\n",
        "            predicted_label = 0\n",
        "            confidence = prob_0\n",
        "\n",
        "    label_map = {0: \"Non-trigger\", 1: \"Trigger\"}\n",
        "\n",
        "    result = {\n",
        "        \"label\": label_map[predicted_label],\n",
        "        \"confidence\": confidence,\n",
        "        \"predicted_digit\": predicted_label\n",
        "    }\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AThnPpWXRfEN",
        "outputId": "c86067f4-6b77-4d92-b747-d89fbdd63580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Testing a few statements:\n",
            "============================================================\n",
            "\n",
            "Statement: I'm feeling much better now\n",
            "Generated: '1'\n",
            "P(0) = 0.3207, P(1) = 0.6789\n",
            "Ratio P(1)/P(0) = 2.12\n",
            "\n",
            "Statement: Everything is fine\n",
            "Generated: '0'\n",
            "P(0) = 0.6222, P(1) = 0.3774\n",
            "Ratio P(1)/P(0) = 0.61\n",
            "\n",
            "Statement: No problems at all\n",
            "Generated: '0'\n",
            "P(0) = 0.5924, P(1) = 0.4072\n",
            "Ratio P(1)/P(0) = 0.69\n",
            "\n",
            "Statement: I feel great\n",
            "Generated: '1'\n",
            "P(0) = 0.4376, P(1) = 0.5619\n",
            "Ratio P(1)/P(0) = 1.28\n",
            "\n",
            "Statement: I have a severe headache and dizziness\n",
            "Generated: '1'\n",
            "P(0) = 0.0534, P(1) = 0.9465\n",
            "Ratio P(1)/P(0) = 17.73\n",
            "\n",
            "Statement: I'm experiencing chest pain\n",
            "Generated: '1'\n",
            "P(0) = 0.4072, P(1) = 0.5925\n",
            "Ratio P(1)/P(0) = 1.45\n",
            "\n",
            "Statement: Ù„Ù‚Ø¯ Ø£ØµØ¨Øª Ø¨Ø·ÙØ­ Ø¬Ù„Ø¯ÙŠ Ø´Ø¯ÙŠØ¯ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¡.\n",
            "Generated: '1'\n",
            "P(0) = 0.0675, P(1) = 0.9323\n",
            "Ratio P(1)/P(0) = 13.80\n",
            "\n",
            "Statement: I'm not feeling well\n",
            "Generated: '1'\n",
            "P(0) = 0.1480, P(1) = 0.8517\n",
            "Ratio P(1)/P(0) = 5.75\n",
            "\n",
            "Statement: The medication is making me sick\n",
            "Generated: '1'\n",
            "P(0) = 0.0758, P(1) = 0.9239\n",
            "Ratio P(1)/P(0) = 12.18\n",
            "\n",
            "Statement: Ø£Ù†Ø§ Ø¨Ø®ÙŠØ±ØŒ Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ\n",
            "Generated: '0'\n",
            "P(0) = 0.4999, P(1) = 0.4999\n",
            "Ratio P(1)/P(0) = 1.00\n",
            "\n",
            "Statement: ÙƒÙ„ Ø´ÙŠØ¡ Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ±Ø§Ù…\n",
            "Generated: '0'\n",
            "P(0) = 0.5620, P(1) = 0.4377\n",
            "Ratio P(1)/P(0) = 0.78\n",
            "\n",
            "Statement: Ø£Ø´Ø¹Ø± Ø¨ØªØ­Ø³Ù† ÙƒØ¨ÙŠØ±\n",
            "Generated: '1'\n",
            "P(0) = 0.2942, P(1) = 0.7056\n",
            "Ratio P(1)/P(0) = 2.40\n",
            "\n",
            "Statement: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§ÙƒÙ„\n",
            "Generated: '0'\n",
            "P(0) = 0.6223, P(1) = 0.3775\n",
            "Ratio P(1)/P(0) = 0.61\n",
            "\n",
            "Statement: Ø§Ù„Ø¯ÙˆØ§Ø¡ ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯\n",
            "Generated: '1'\n",
            "P(0) = 0.1480, P(1) = 0.8519\n",
            "Ratio P(1)/P(0) = 5.75\n",
            "\n",
            "Statement: Ø£Ø´Ø¹Ø± Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©\n",
            "Generated: '0'\n",
            "P(0) = 0.5311, P(1) = 0.4687\n",
            "Ratio P(1)/P(0) = 0.88\n",
            "\n",
            "Statement: Ù„Ù… Ø£Ø¹Ø¯ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø£Ù„Ù…\n",
            "Generated: '1'\n",
            "P(0) = 0.2451, P(1) = 0.7548\n",
            "Ratio P(1)/P(0) = 3.08\n",
            "\n",
            "Statement: ØªØ¹Ø§ÙÙŠØª ØªÙ…Ø§Ù…Ø§Ù‹\n",
            "Generated: '1'\n",
            "P(0) = 0.4377, P(1) = 0.5621\n",
            "Ratio P(1)/P(0) = 1.28\n",
            "\n",
            "Statement: Ø£Ø´Ø¹Ø± Ø¨ØªØ­Ø³Ù† ÙƒÙ„ ÙŠÙˆÙ…\n",
            "Generated: '1'\n",
            "P(0) = 0.2941, P(1) = 0.7056\n",
            "Ratio P(1)/P(0) = 2.40\n",
            "\n",
            "Statement: Ù„Ø§ Ø£Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø£ÙŠ Ø£Ø¹Ø±Ø§Ø¶\n",
            "Generated: '0'\n",
            "P(0) = 0.5925, P(1) = 0.4072\n",
            "Ratio P(1)/P(0) = 0.69\n",
            "\n",
            "Statement: ØµØ­ØªÙŠ Ù…Ù…ØªØ§Ø²Ø© Ø§Ù„Ø¢Ù†\n",
            "Generated: '0'\n",
            "P(0) = 0.4999, P(1) = 0.4999\n",
            "Ratio P(1)/P(0) = 1.00\n",
            "\n",
            "Statement: Ø§Ù„Ø£Ù…ÙˆØ± ØªØ³ÙŠØ± Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ±Ø§Ù…\n",
            "Generated: '0'\n",
            "P(0) = 0.5925, P(1) = 0.4072\n",
            "Ratio P(1)/P(0) = 0.69\n",
            "\n",
            "Statement: Ø£Ù†Ø§ Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ± Ø§Ù„Ø¢Ù†\n",
            "Generated: '1'\n",
            "P(0) = 0.3486, P(1) = 0.6512\n",
            "Ratio P(1)/P(0) = 1.87\n",
            "\n",
            "Statement: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø£Ù„Ù…\n",
            "Generated: '1'\n",
            "P(0) = 0.4377, P(1) = 0.5621\n",
            "Ratio P(1)/P(0) = 1.28\n",
            "\n",
            "Statement: Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø©\n",
            "Generated: '1'\n",
            "P(0) = 0.2942, P(1) = 0.7056\n",
            "Ratio P(1)/P(0) = 2.40\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Diagnostic Code (Fixed)\n",
        "# -----------------------------\n",
        "\n",
        "test_statements = [\n",
        "    \"I'm feeling much better now\",\n",
        "    \"Everything is fine\",\n",
        "    \"No problems at all\",\n",
        "    \"I feel great\",\n",
        "    \"I have a severe headache and dizziness\",\n",
        "    \"I'm experiencing chest pain\",\n",
        "    \"Ù„Ù‚Ø¯ Ø£ØµØ¨Øª Ø¨Ø·ÙØ­ Ø¬Ù„Ø¯ÙŠ Ø´Ø¯ÙŠØ¯ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¡.\",\n",
        "    \"I'm not feeling well\",\n",
        "    \"The medication is making me sick\",\n",
        "    \"Ø£Ù†Ø§ Ø¨Ø®ÙŠØ±ØŒ Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ\",  # I'm fine, thank you\n",
        "    \"ÙƒÙ„ Ø´ÙŠØ¡ Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ±Ø§Ù…\",  # Everything is okay\n",
        "    \"Ø£Ø´Ø¹Ø± Ø¨ØªØ­Ø³Ù† ÙƒØ¨ÙŠØ±\",  # I feel much better\n",
        "    \"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§ÙƒÙ„\",  # No problems\n",
        "    \"Ø§Ù„Ø¯ÙˆØ§Ø¡ ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯\",  # The medication is working well\n",
        "    \"Ø£Ø´Ø¹Ø± Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©\",  # I feel healthy\n",
        "    \"Ù„Ù… Ø£Ø¹Ø¯ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø£Ù„Ù…\",  # I no longer feel pain\n",
        "    \"ØªØ¹Ø§ÙÙŠØª ØªÙ…Ø§Ù…Ø§Ù‹\",  # I've recovered completely\n",
        "    \"Ø£Ø´Ø¹Ø± Ø¨ØªØ­Ø³Ù† ÙƒÙ„ ÙŠÙˆÙ…\",  # I feel better every day\n",
        "    \"Ù„Ø§ Ø£Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø£ÙŠ Ø£Ø¹Ø±Ø§Ø¶\",  # I don't have any symptoms\n",
        "    \"ØµØ­ØªÙŠ Ù…Ù…ØªØ§Ø²Ø© Ø§Ù„Ø¢Ù†\",  # My health is excellent now\n",
        "    \"Ø§Ù„Ø£Ù…ÙˆØ± ØªØ³ÙŠØ± Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ±Ø§Ù…\",  # Things are going well\n",
        "    \"Ø£Ù†Ø§ Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ± Ø§Ù„Ø¢Ù†\",  # I'm much better now\n",
        "    \"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø£Ù„Ù…\",  # There is no pain\n",
        "    \"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø©\",  # I feel comfortable\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing a few statements:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for statement in test_statements:\n",
        "    prompt = (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"Classify the following patient statement as 0 (Not Triggered) or 1 (Triggered).\\n\"\n",
        "        \"Respond with exactly one character: 0 or 1.\\n\\n\"\n",
        "        \"### Statement:\\n\"\n",
        "        f\"{statement}\\n\\n\"\n",
        "        \"### Response:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate with output_scores to get probabilities\n",
        "        output = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=1,\n",
        "            do_sample=False,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True\n",
        "        )\n",
        "\n",
        "        output_ids = output.sequences\n",
        "        scores = output.scores[0][0]  # Logits for the first (and only) generated token\n",
        "\n",
        "        predicted_token = tokenizer.decode(\n",
        "            output_ids[0, inputs.input_ids.shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "\n",
        "        # Get token IDs for \"0\" and \"1\"\n",
        "        token_0 = tokenizer.encode(\"0\", add_special_tokens=False)[0]\n",
        "        token_1 = tokenizer.encode(\"1\", add_special_tokens=False)[0]\n",
        "\n",
        "        # Get probabilities\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "        prob_0 = probs[token_0].item()\n",
        "        prob_1 = probs[token_1].item()\n",
        "\n",
        "    print(f\"\\nStatement: {statement}\")\n",
        "    print(f\"Generated: '{predicted_token}'\")\n",
        "    print(f\"P(0) = {prob_0:.4f}, P(1) = {prob_1:.4f}\")\n",
        "    if prob_0 > 0:\n",
        "        print(f\"Ratio P(1)/P(0) = {prob_1/prob_0:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq9QWCKvTzuI",
        "outputId": "e8aa954f-9721-4961-d5ae-459b5be0dcab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6rD4E0c3Zi-"
      },
      "outputs": [],
      "source": [
        "CSV_PATH = \"/content/drive/MyDrive/Memo_Dataset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gDo1qLQuUMpY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7N4cUYeTvD7",
        "outputId": "61c71409-cb89-457f-f2b2-09d37c75843d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw dataset size: 13142\n",
            "label\n",
            "1    7687\n",
            "0    5455\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset size: 10910\n",
            "label\n",
            "0    5455\n",
            "1    5455\n",
            "Name: count, dtype: int64\n",
            "Train: 8728, Test: 2182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4211767844.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_bal = df.groupby('label', group_keys=False).apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(CSV_PATH)\n",
        "# keep only columns we need\n",
        "#df = df[['Question', 'Trigger']].dropna() #arabic column\n",
        "df = df[['Question_eng', 'Trigger']].dropna() #english column\n",
        "#df = df.rename(columns={'Question': 'text', 'Trigger': 'label'})\n",
        "df = df.rename(columns={'Question_eng': 'text', 'Trigger': 'label'})\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "print(\"Raw dataset size:\", len(df))\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Balance classes by downsampling the majority class\n",
        "min_count = df['label'].value_counts().min()\n",
        "df_bal = df.groupby('label', group_keys=False).apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n",
        "print(\"Balanced dataset size:\", len(df_bal))\n",
        "print(df_bal['label'].value_counts())\n",
        "\n",
        "train_df, test_df = train_test_split(df_bal, test_size=0.2, random_state=42, stratify=df_bal['label'])\n",
        "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoLrEgoObsPc",
        "outputId": "e334f021-7cea-4e00-d60b-7667bb70e1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "0    1091\n",
            "1    1091\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(test_df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8jL7qQcUi1x",
        "outputId": "65e330d6-8ec3-425f-e6e6-ac5f17c46545"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from datetime import datetime\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# isTrigger function with probabilities\n",
        "# -----------------------------\n",
        "def isTrigger(text: str):\n",
        "    \"\"\"\n",
        "    Classify patient statement as trigger (1) or non-trigger (0).\n",
        "\n",
        "    Args:\n",
        "        text: Patient statement in English or Arabic\n",
        "\n",
        "    Returns:\n",
        "        dict with 'label' (0 or 1) and 'confidence' (probability)\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"Classify the following patient statement as 0 (Not Triggered) or 1 (Triggered).\\n\"\n",
        "        \"Respond with exactly one character: 0 or 1.\\n\\n\"\n",
        "        \"### Statement:\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "        \"### Response:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate with scores\n",
        "        output = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=1,\n",
        "            do_sample=False,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True\n",
        "        )\n",
        "\n",
        "        output_ids = output.sequences\n",
        "        scores = output.scores[0][0]  # Logits for the generated token\n",
        "\n",
        "        predicted_token = tokenizer.decode(\n",
        "            output_ids[0, inputs.input_ids.shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "\n",
        "        # Get token IDs for \"0\" and \"1\"\n",
        "        token_0 = tokenizer.encode(\"0\", add_special_tokens=False)[0]\n",
        "        token_1 = tokenizer.encode(\"1\", add_special_tokens=False)[0]\n",
        "\n",
        "        # Get probabilities\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "        prob_0 = probs[token_0].item()\n",
        "        prob_1 = probs[token_1].item()\n",
        "\n",
        "    # Extract predicted label\n",
        "    predicted_label = None\n",
        "    for ch in predicted_token:\n",
        "        if ch in (\"0\", \"1\"):\n",
        "            predicted_label = int(ch)\n",
        "            break\n",
        "\n",
        "    if predicted_label is None:\n",
        "        print(f\"Warning: Model generated '{predicted_token}' instead of 0 or 1\")\n",
        "        predicted_label = 0\n",
        "\n",
        "    # Get confidence for the predicted label\n",
        "    confidence = prob_0 if predicted_label == 0 else prob_1\n",
        "\n",
        "    result = {\n",
        "        \"label\": predicted_label,\n",
        "        \"confidence\": confidence,\n",
        "        \"prob_0\": prob_0,\n",
        "        \"prob_1\": prob_1\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluate on test set\n",
        "# -----------------------------\n",
        "print(\"=\"*60)\n",
        "print(f\"Evaluating on test set ({len(test_df)} samples)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "predictions = []\n",
        "confidences = []\n",
        "prob_0_list = []\n",
        "prob_1_list = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    statement = row['text']\n",
        "    result = isTrigger(statement)\n",
        "    predictions.append(result['label'])\n",
        "    confidences.append(result['confidence'])\n",
        "    prob_0_list.append(result['prob_0'])\n",
        "    prob_1_list.append(result['prob_1'])\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "\n",
        "# Create results dataframe\n",
        "test_results_df = test_df.copy()\n",
        "test_results_df['prediction'] = predictions\n",
        "test_results_df['confidence'] = confidences\n",
        "test_results_df['prob_0'] = prob_0_list\n",
        "test_results_df['prob_1'] = prob_1_list\n",
        "test_results_df['correct'] = test_results_df['label'] == test_results_df['prediction']\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(test_results_df['label'], predictions)\n",
        "precision = precision_score(test_results_df['label'], predictions)\n",
        "recall = recall_score(test_results_df['label'], predictions)\n",
        "f1 = f1_score(test_results_df['label'], predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST SET RESULTS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"=\"*60)\n",
        "cm = confusion_matrix(test_results_df['label'], predictions)\n",
        "print(f\"                Predicted 0  Predicted 1\")\n",
        "print(f\"Actual 0        {cm[0][0]:>11}  {cm[0][1]:>11}\")\n",
        "print(f\"Actual 1        {cm[1][0]:>11}  {cm[1][1]:>11}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(test_results_df['label'], predictions,\n",
        "                          target_names=['Non-trigger (0)', 'Trigger (1)']))\n",
        "\n",
        "# Average confidence for correct vs incorrect predictions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Confidence Analysis:\")\n",
        "print(\"=\"*60)\n",
        "correct_df = test_results_df[test_results_df['correct'] == True]\n",
        "incorrect_df = test_results_df[test_results_df['correct'] == False]\n",
        "print(f\"Average confidence (correct predictions):   {correct_df['confidence'].mean():.4f}\")\n",
        "print(f\"Average confidence (incorrect predictions): {incorrect_df['confidence'].mean():.4f}\")\n",
        "\n",
        "# Save predictions\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = f\"/content/drive/MyDrive/test_predictions_{timestamp}.csv\"\n",
        "test_results_df.to_csv(save_path, index=False)\n",
        "print(f\"\\nPredictions saved to: {save_path}\")\n",
        "\n",
        "# Show misclassifications with probabilities\n",
        "misclassified = test_results_df[test_results_df['label'] != test_results_df['prediction']]\n",
        "if len(misclassified) > 0:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Misclassifications: {len(misclassified)}/{len(test_df)}\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nFirst 10 examples:\")\n",
        "    for idx, row in misclassified.head(10).iterrows():\n",
        "        print(f\"\\nStatement: {row['text']}\")\n",
        "        print(f\"True: {row['label']}, Predicted: {row['prediction']}\")\n",
        "        print(f\"P(0)={row['prob_0']:.4f}, P(1)={row['prob_1']:.4f}, Confidence={row['confidence']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0DEVwIRWwXy",
        "outputId": "3569a4d0-5651-435d-8332-16ff0eea615d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# -----------------------------\n",
        "# Save metrics in structured format for later use\n",
        "# -----------------------------\n",
        "\n",
        "# Create metrics dictionary\n",
        "metrics_dict = {\n",
        "    'model_name': 'llama-3b-instruct-arabic',\n",
        "    'timestamp': timestamp,\n",
        "    'dataset_info': {\n",
        "        'total_samples': len(test_df),\n",
        "        'class_0_samples': int((test_results_df['label'] == 0).sum()),\n",
        "        'class_1_samples': int((test_results_df['label'] == 1).sum()),\n",
        "    },\n",
        "    'metrics': {\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1),\n",
        "    },\n",
        "    'confusion_matrix': {\n",
        "        'true_negative': int(cm[0][0]),   # Actual 0, Predicted 0\n",
        "        'false_positive': int(cm[0][1]),  # Actual 0, Predicted 1\n",
        "        'false_negative': int(cm[1][0]),  # Actual 1, Predicted 0\n",
        "        'true_positive': int(cm[1][1]),   # Actual 1, Predicted 1\n",
        "    },\n",
        "    'per_class_metrics': {\n",
        "        'class_0': {\n",
        "            'precision': float(precision_score(test_results_df['label'], predictions, pos_label=0)),\n",
        "            'recall': float(recall_score(test_results_df['label'], predictions, pos_label=0)),\n",
        "            'f1_score': float(f1_score(test_results_df['label'], predictions, pos_label=0)),\n",
        "        },\n",
        "        'class_1': {\n",
        "            'precision': float(precision),  # Already calculated for class 1\n",
        "            'recall': float(recall),\n",
        "            'f1_score': float(f1),\n",
        "        }\n",
        "    },\n",
        "    'confidence_analysis': {\n",
        "        'avg_confidence_correct': float(correct_df['confidence'].mean()),\n",
        "        'avg_confidence_incorrect': float(incorrect_df['confidence'].mean()),\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save as JSON (complete detail)\n",
        "json_path = f\"/content/drive/MyDrive/metrics_llama3b_{timestamp}.json\"\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(metrics_dict, f, indent=2)\n",
        "print(f\"Metrics JSON saved to: {json_path}\")\n",
        "\n",
        "# Save as single-row CSV (easy to combine with other models)\n",
        "metrics_csv = pd.DataFrame([{\n",
        "    'model': 'llama-3b-instruct-arabic',\n",
        "    'timestamp': timestamp,\n",
        "    'total_samples': len(test_df),\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1_score': f1,\n",
        "    'TN': cm[0][0],\n",
        "    'FP': cm[0][1],\n",
        "    'FN': cm[1][0],\n",
        "    'TP': cm[1][1],\n",
        "    'class_0_precision': precision_score(test_results_df['label'], predictions, pos_label=0),\n",
        "    'class_0_recall': recall_score(test_results_df['label'], predictions, pos_label=0),\n",
        "    'class_0_f1': f1_score(test_results_df['label'], predictions, pos_label=0),\n",
        "    'class_1_precision': precision,\n",
        "    'class_1_recall': recall,\n",
        "    'class_1_f1': f1,\n",
        "    'avg_conf_correct': correct_df['confidence'].mean(),\n",
        "    'avg_conf_incorrect': incorrect_df['confidence'].mean(),\n",
        "}])\n",
        "\n",
        "csv_path = f\"/content/drive/MyDrive/metrics_llama3b_{timestamp}.csv\"\n",
        "metrics_csv.to_csv(csv_path, index=False)\n",
        "print(f\"Metrics CSV saved to: {csv_path}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helper function to generate LaTeX table later\n",
        "# -----------------------------\n",
        "def create_latex_table(csv_files_list):\n",
        "    \"\"\"\n",
        "    Combine multiple model metrics CSVs into a LaTeX table.\n",
        "\n",
        "    Args:\n",
        "        csv_files_list: List of paths to metrics CSV files\n",
        "\n",
        "    Returns:\n",
        "        String containing LaTeX table code\n",
        "    \"\"\"\n",
        "    # Combine all CSVs\n",
        "    dfs = [pd.read_csv(f) for f in csv_files_list]\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Select key columns for the table\n",
        "    table_df = combined_df[['model', 'accuracy', 'precision', 'recall', 'f1_score']]\n",
        "\n",
        "    # Round to 3 decimal places\n",
        "    for col in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
        "        table_df[col] = table_df[col].round(3)\n",
        "\n",
        "    # Generate LaTeX\n",
        "    latex_str = table_df.to_latex(index=False, escape=False,\n",
        "                                   column_format='l|cccc',\n",
        "                                   caption='Model Performance Comparison',\n",
        "                                   label='tab:model_comparison')\n",
        "\n",
        "    return latex_str\n",
        "\n",
        "# Example usage (you'll do this later when you have multiple models):\n",
        "# latex_table = create_latex_table([\n",
        "#     '/path/to/metrics_llama3b_20241125_120000.csv',\n",
        "#     '/path/to/metrics_xlmr_20241125_130000.csv',\n",
        "#     '/path/to/metrics_other_model.csv'\n",
        "# ])\n",
        "# print(latex_table)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a7534461fd94b9f8ec5722df163cf58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3d012f9279b4d5a99bd2a7be67f4453",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8cc9cabb6c7a4b74b6e5cbbe1f034b41",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "1c612521a0c34204b4bfd73856ad4c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1eb630edc237401595c86cd685027699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a7534461fd94b9f8ec5722df163cf58",
              "IPY_MODEL_2d60c292d7d44f25a1862e85dd637cff",
              "IPY_MODEL_786555df799b4fa0b563781d1e6103bb"
            ],
            "layout": "IPY_MODEL_75ab2321f8434fda8debb1bf9777663c"
          }
        },
        "2d60c292d7d44f25a1862e85dd637cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8847fbc5ade84e1bb0085df55602e5db",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c612521a0c34204b4bfd73856ad4c53",
            "value": 4
          }
        },
        "40603c456cdb4c49bc4d007bdcaccd06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ab2321f8434fda8debb1bf9777663c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786555df799b4fa0b563781d1e6103bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40603c456cdb4c49bc4d007bdcaccd06",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f7c8938744d14b9cae922e168244bfd3",
            "value": "â€‡4/4â€‡[00:17&lt;00:00,â€‡â€‡3.62s/it]"
          }
        },
        "8847fbc5ade84e1bb0085df55602e5db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cc9cabb6c7a4b74b6e5cbbe1f034b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3d012f9279b4d5a99bd2a7be67f4453": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c8938744d14b9cae922e168244bfd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
