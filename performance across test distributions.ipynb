{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629301be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5364a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    'Llama3-8B-Arabic': {\n",
    "        'arabic': 'memo-llama3-8b-instruct-arabic_evaluation_results/test_predictions_20251125_051031.csv',\n",
    "        'english': 'memo-llama3-8b-instruct-arabic_evaluation_results/test_predictions_20260110_091937.csv',\n",
    "        'label_col': 'label',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "     'Meta-Llama-3-8B-Instruct': {\n",
    "        'arabic': 'meta-llama-3-8b-instruct\\llama_3b_base_test_predictions_arabic_20260202_112839.csv',\n",
    "        'english': 'meta-llama-3-8b-instruct\\llama_3b_base_test_predictions_eng_20260202_110950.csv',\n",
    "        'label_col': 'label',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "    'Claude-Haiku-4.5': {\n",
    "        'arabic': 'gpt and claude eval/Claude-Haiku-4.5_Arabic_20251210_104613.csv',\n",
    "        'english': 'gpt and claude eval/Claude-Haiku-4.5_English_20251210_112951.csv',\n",
    "        'label_col': 'Trigger',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "    'GPT-4o-mini': {\n",
    "        'arabic': 'gpt and claude eval/GPT-4o-mini_Arabic_20251210_084029.csv',\n",
    "        'english': 'gpt and claude eval/GPT-4o-mini_English_20251210_091410.csv',\n",
    "        'label_col': 'Trigger',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "    'XLM-RoBERTa': {\n",
    "        'arabic': 'zeroshot_base_xlmr_roberta_results/ZeroShot_base_trigger_non-trigger_Arabic_20251125_042246.csv',\n",
    "        'english': 'zeroshot_base_xlmr_roberta_results/ZeroShot_base_trigger_non-trigger_English_20251125_042345.csv',\n",
    "        'label_col': 'Trigger',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "    'xlm-roberta-large-xnli': {\n",
    "        'arabic': 'zeroshot_baseline_xlm_roberta_nli_results/ZeroShot_trigger_non-trigger_Arabic_20251125_035721.csv',\n",
    "        'english': 'zeroshot_baseline_xlm_roberta_nli_results/ZeroShot_trigger_non-trigger_English_20251125_040107.csv',\n",
    "        'label_col': 'Trigger',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "    'XLM-R-Arabic-FT': {\n",
    "        'arabic': 'xlmr_roberta_evaluation_results/XLM-R-Arabic_Arabic_20251125_022752.csv',\n",
    "        'english': 'xlmr_roberta_evaluation_results/XLM-R-Arabic_English_20251125_022759.csv',\n",
    "        'label_col': 'Trigger',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "    'XLM-R-English-FT': {\n",
    "        'arabic': 'xlmr_roberta_evaluation_results/XLM-R-English_Arabic_20251125_022832.csv',\n",
    "        'english': 'xlmr_roberta_evaluation_results/XLM-R-English_English_20251125_022839.csv',\n",
    "        'label_col': 'Trigger',\n",
    "        'pred_col': 'prediction'\n",
    "    },\n",
    "    'XLM-R-Multilingual-FT': {\n",
    "        'arabic': 'xlmr_roberta_evaluation_results/XLM-R-Multilingual_Arabic_20251125_022713.csv',\n",
    "        'english': 'xlmr_roberta_evaluation_results/XLM-R-Multilingual_English_20251125_022720.csv',\n",
    "        'label_col': 'Trigger',\n",
    "        'pred_col': 'prediction'\n",
    "    }\n",
    "}\n",
    "\n",
    "#sampling parameters\n",
    "TARGET_DISTRIBUTIONS = np.arange(0.05, 1.0, 0.05)\n",
    "SAMPLE_SIZES = [300, 500, 700]\n",
    "N_ITERATIONS = 200\n",
    "CONFIDENCE_LEVEL = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8adf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(model_name, language, config):\n",
    "    \"\"\"Load predictions for a specific model and language.\"\"\"\n",
    "    base_dir = os.getcwd()\n",
    "    filepath = os.path.join(base_dir, config[language])\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    true_labels = df[config['label_col']].values\n",
    "    predictions = df[config['pred_col']].values\n",
    "    \n",
    "    return true_labels, predictions, df\n",
    "\n",
    "def stratified_sample_by_distribution(true_labels, predictions, target_prop_class1, sample_size, df=None):\n",
    "    #samples data to achieve some target distribution for class 1\n",
    "    class1_idx = np.where(true_labels == 1)[0]\n",
    "    class0_idx = np.where(true_labels == 0)[0]\n",
    "    \n",
    "    n_class1 = int(sample_size * target_prop_class1)\n",
    "    n_class0 = sample_size - n_class1\n",
    "    \n",
    "    if n_class1 > len(class1_idx) or n_class0 > len(class0_idx):\n",
    "        return None, None, None\n",
    "    \n",
    "    sampled_class1_idx = np.random.choice(class1_idx, size=n_class1, replace=False)\n",
    "    sampled_class0_idx = np.random.choice(class0_idx, size=n_class0, replace=False)\n",
    "    \n",
    "    sampled_idx = np.concatenate([sampled_class1_idx, sampled_class0_idx])\n",
    "    np.random.shuffle(sampled_idx)\n",
    "    \n",
    "    return true_labels[sampled_idx], predictions[sampled_idx], sampled_idx\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    #Calculates all performance metrics\n",
    "    return {\n",
    "        'accuracy': accuracy_score(true_labels, predictions),\n",
    "        'precision': precision_score(true_labels, predictions, zero_division=0),\n",
    "        'recall': recall_score(true_labels, predictions, zero_division=0),\n",
    "        'f1': f1_score(true_labels, predictions, zero_division=0)\n",
    "    }\n",
    "\n",
    "def calculate_confidence_interval(values, confidence=0.95):\n",
    "    #Calculates confidence interval for a list of values\n",
    "    n = len(values)\n",
    "    if n == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    mean = np.mean(values)\n",
    "    std_err = stats.sem(values)\n",
    "    margin = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    return mean - margin, mean + margin\n",
    "\n",
    "def analyze_prediction_distributions(model_name, language, config):\n",
    "    #Analyzes the distribution of predictions made by a model\n",
    "    print(f\"Analyzing prediction distribution for {model_name} ({language})...\")\n",
    "    \n",
    "    true_labels, predictions, df = load_model_data(model_name, language, config)\n",
    "    \n",
    "    total_samples = len(predictions)\n",
    "    class1_predictions = np.sum(predictions == 1)\n",
    "    class0_predictions = np.sum(predictions == 0)\n",
    "    \n",
    "    class1_true = np.sum(true_labels == 1)\n",
    "    class0_true = np.sum(true_labels == 0)\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'language': language,\n",
    "        'total_samples': total_samples,\n",
    "        'true_class1_count': class1_true,\n",
    "        'true_class1_proportion': class1_true / total_samples,\n",
    "        'true_class0_count': class0_true,\n",
    "        'true_class0_proportion': class0_true / total_samples,\n",
    "        'pred_class1_count': class1_predictions,\n",
    "        'pred_class1_proportion': class1_predictions / total_samples,\n",
    "        'pred_class0_count': class0_predictions,\n",
    "        'pred_class0_proportion': class0_predictions / total_samples,\n",
    "        'prediction_bias': (class1_predictions / total_samples) - (class1_true / total_samples)\n",
    "    }\n",
    "\n",
    "def evaluate_across_distributions(model_name, language, config, sample_size, save_samples=True):\n",
    "    #Evaluates a model across different label distributions\n",
    "    print(f\"Evaluating {model_name} ({language}) with sample_size={sample_size}...\")\n",
    "    \n",
    "    true_labels, predictions, df = load_model_data(model_name, language, config)\n",
    "    \n",
    "    results = []\n",
    "    sampled_indices_tracker = []\n",
    "    \n",
    "    for target_dist in TARGET_DISTRIBUTIONS:\n",
    "        dist_metrics = {metric: [] for metric in ['accuracy', 'precision', 'recall', 'f1']}\n",
    "        iteration_samples = []\n",
    "        \n",
    "        for iteration in range(N_ITERATIONS):\n",
    "            sampled_true, sampled_pred, sampled_idx = stratified_sample_by_distribution(\n",
    "                true_labels, predictions, target_dist, sample_size, df\n",
    "            )\n",
    "            \n",
    "            if sampled_true is None:\n",
    "                continue\n",
    "            \n",
    "            if save_samples and iteration < 10:\n",
    "                iteration_samples.append({\n",
    "                    'iteration': iteration,\n",
    "                    'indices': sampled_idx.tolist()\n",
    "                })\n",
    "            \n",
    "            metrics = calculate_metrics(sampled_true, sampled_pred)\n",
    "            \n",
    "            for metric_name, value in metrics.items():\n",
    "                dist_metrics[metric_name].append(value)\n",
    "        \n",
    "        if save_samples and iteration_samples:\n",
    "            sampled_indices_tracker.append({\n",
    "                'model': model_name,\n",
    "                'language': language,\n",
    "                'sample_size': sample_size,\n",
    "                'target_distribution': target_dist,\n",
    "                'iterations': iteration_samples\n",
    "            })\n",
    "        \n",
    "        if dist_metrics['accuracy']:\n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'language': language,\n",
    "                'sample_size': sample_size,\n",
    "                'distribution': target_dist,\n",
    "            }\n",
    "            \n",
    "            for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "                values = dist_metrics[metric]\n",
    "                ci_lower, ci_upper = calculate_confidence_interval(values, CONFIDENCE_LEVEL)\n",
    "                \n",
    "                result[f'{metric}_mean'] = np.mean(values)\n",
    "                result[f'{metric}_std'] = np.std(values)\n",
    "                result[f'{metric}_ci_lower'] = ci_lower\n",
    "                result[f'{metric}_ci_upper'] = ci_upper\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results), sampled_indices_tracker\n",
    "\n",
    "def run_full_evaluation(save_samples=True):\n",
    "    \"\"\"Run evaluation for all models, languages, and sample sizes.\"\"\"\n",
    "    all_results = []\n",
    "    all_sampled_indices = []\n",
    "    all_prediction_distributions = []\n",
    "    \n",
    "    total_runs = len(MODEL_CONFIGS) * 2 * len(SAMPLE_SIZES)\n",
    "    current_run = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYZING PREDICTION DISTRIBUTIONS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for model_name, config in MODEL_CONFIGS.items():\n",
    "        for language in ['arabic', 'english']:\n",
    "            try:\n",
    "                dist_info = analyze_prediction_distributions(model_name, language, config)\n",
    "                all_prediction_distributions.append(dist_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {model_name} ({language}): {e}\")\n",
    "    \n",
    "    pred_dist_df = pd.DataFrame(all_prediction_distributions)\n",
    "    base_dir = os.getcwd()\n",
    "    pred_dist_path = os.path.join(base_dir, 'model_prediction_distributions.csv')\n",
    "    pred_dist_df.to_csv(pred_dist_path, index=False)\n",
    "    print(f\"\\nPrediction distributions saved to '{pred_dist_path}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING MAIN EVALUATION\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for model_name, config in MODEL_CONFIGS.items():\n",
    "        for language in ['arabic', 'english']:\n",
    "            for sample_size in SAMPLE_SIZES:\n",
    "                current_run += 1\n",
    "                print(f\"\\n[{current_run}/{total_runs}]\", end=\" \")\n",
    "                try:\n",
    "                    results_df, sampled_indices = evaluate_across_distributions(\n",
    "                        model_name, language, config, sample_size, save_samples\n",
    "                    )\n",
    "                    all_results.append(results_df)\n",
    "                    all_sampled_indices.extend(sampled_indices)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {model_name} ({language}, size={sample_size}): {e}\")\n",
    "    \n",
    "    if save_samples and all_sampled_indices:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        samples_path = os.path.join(base_dir, f'sampled_indices_{timestamp}.json')\n",
    "        with open(samples_path, 'w') as f:\n",
    "            json.dump(all_sampled_indices, f, indent=2)\n",
    "        print(f\"\\nSampled indices saved to '{samples_path}'\")\n",
    "    \n",
    "    final_results = pd.concat(all_results, ignore_index=True)\n",
    "    return final_results, pred_dist_df\n",
    "\n",
    "def plot_prediction_distributions(pred_dist_df, save_path='prediction_distributions.png'):\n",
    "    #visualizes the prediction distributions across models\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    arabic_data = pred_dist_df[pred_dist_df['language'] == 'arabic'].sort_values('prediction_bias')\n",
    "    \n",
    "    y_pos_ar = np.arange(len(arabic_data))\n",
    "    colors_ar = ['red' if x < 0 else 'green' for x in arabic_data['prediction_bias']]\n",
    "    \n",
    "    ax1.barh(y_pos_ar, arabic_data['prediction_bias'], color=colors_ar, alpha=0.6)\n",
    "    ax1.set_yticks(y_pos_ar)\n",
    "    ax1.set_yticklabels(arabic_data['model'], fontsize=9)\n",
    "    ax1.set_xlabel('Prediction Bias (Predicted - True Class 1 Proportion)', fontsize=11)\n",
    "    ax1.set_title('Arabic: Model Prediction Bias', fontsize=12, fontweight='bold')\n",
    "    ax1.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    english_data = pred_dist_df[pred_dist_df['language'] == 'english'].sort_values('prediction_bias')\n",
    "    \n",
    "    y_pos_en = np.arange(len(english_data))\n",
    "    colors_en = ['red' if x < 0 else 'green' for x in english_data['prediction_bias']]\n",
    "    \n",
    "    ax2.barh(y_pos_en, english_data['prediction_bias'], color=colors_en, alpha=0.6)\n",
    "    ax2.set_yticks(y_pos_en)\n",
    "    ax2.set_yticklabels(english_data['model'], fontsize=9)\n",
    "    ax2.set_xlabel('Prediction Bias (Predicted - True Class 1 Proportion)', fontsize=11)\n",
    "    ax2.set_title('English: Model Prediction Bias', fontsize=12, fontweight='bold')\n",
    "    ax2.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    models = arabic_data['model'].values\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, arabic_data['true_class1_proportion'], width, \n",
    "            label='True Class 1', alpha=0.8, color='blue')\n",
    "    ax3.bar(x + width/2, arabic_data['pred_class1_proportion'], width,\n",
    "            label='Predicted Class 1', alpha=0.8, color='orange')\n",
    "    ax3.set_ylabel('Proportion', fontsize=11)\n",
    "    ax3.set_title('Arabic: True vs Predicted Class 1 Distribution', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(models, rotation=45, ha='right', fontsize=8)\n",
    "    ax3.legend(fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    ax3.set_ylim([0, 1])\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    models = english_data['model'].values\n",
    "    x = np.arange(len(models))\n",
    "    \n",
    "    ax4.bar(x - width/2, english_data['true_class1_proportion'], width,\n",
    "            label='True Class 1', alpha=0.8, color='blue')\n",
    "    ax4.bar(x + width/2, english_data['pred_class1_proportion'], width,\n",
    "            label='Predicted Class 1', alpha=0.8, color='orange')\n",
    "    ax4.set_ylabel('Proportion', fontsize=11)\n",
    "    ax4.set_title('English: True vs Predicted Class 1 Distribution', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(models, rotation=45, ha='right', fontsize=8)\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    ax4.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nPrediction distribution plot saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def print_prediction_distribution_summary(pred_dist_df):\n",
    "    \"\"\"Print a summary table of prediction distributions.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL PREDICTION DISTRIBUTION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Model':<25} {'Lang':<7} {'True C1%':<10} {'Pred C1%':<10} {'Bias':<10} {'Total N'}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for _, row in pred_dist_df.iterrows():\n",
    "        print(f\"{row['model']:<25} {row['language']:<7} \"\n",
    "              f\"{row['true_class1_proportion']:>8.1%}   \"\n",
    "              f\"{row['pred_class1_proportion']:>8.1%}   \"\n",
    "              f\"{row['prediction_bias']:>8.3f}   \"\n",
    "              f\"{row['total_samples']:>6}\")\n",
    "\n",
    "def plot_results_by_sample_size(results_df, metric='f1', save_path='distribution_analysis_by_sample_size.png'):\n",
    "    #creates visualization comparing different sample sizes\n",
    "    fig, axes = plt.subplots(len(SAMPLE_SIZES), 2, figsize=(16, 6 * len(SAMPLE_SIZES)))\n",
    "    \n",
    "    if len(SAMPLE_SIZES) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, sample_size in enumerate(SAMPLE_SIZES):\n",
    "        ax_ar = axes[idx, 0]\n",
    "        arabic_data = results_df[(results_df['language'] == 'arabic') & \n",
    "                                 (results_df['sample_size'] == sample_size)]\n",
    "        \n",
    "        for model in arabic_data['model'].unique():\n",
    "            model_data = arabic_data[arabic_data['model'] == model]\n",
    "            ax_ar.plot(model_data['distribution'], model_data[f'{metric}_mean'], \n",
    "                      marker='o', label=model, linewidth=2)\n",
    "            ax_ar.fill_between(model_data['distribution'], \n",
    "                              model_data[f'{metric}_ci_lower'],\n",
    "                              model_data[f'{metric}_ci_upper'],\n",
    "                              alpha=0.2)\n",
    "        \n",
    "        ax_ar.set_xlabel('Proportion of Class 1 (Triggering)', fontsize=12)\n",
    "        ax_ar.set_ylabel(f'{metric.upper()} Score', fontsize=12)\n",
    "        ax_ar.set_title(f'Arabic - Sample Size {sample_size} - {metric.upper()}', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "        ax_ar.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        ax_ar.grid(True, alpha=0.3)\n",
    "        ax_ar.set_ylim([0, 1.05])\n",
    "        \n",
    "        ax_en = axes[idx, 1]\n",
    "        english_data = results_df[(results_df['language'] == 'english') & \n",
    "                                  (results_df['sample_size'] == sample_size)]\n",
    "        \n",
    "        for model in english_data['model'].unique():\n",
    "            model_data = english_data[english_data['model'] == model]\n",
    "            ax_en.plot(model_data['distribution'], model_data[f'{metric}_mean'], \n",
    "                      marker='o', label=model, linewidth=2)\n",
    "            ax_en.fill_between(model_data['distribution'], \n",
    "                              model_data[f'{metric}_ci_lower'],\n",
    "                              model_data[f'{metric}_ci_upper'],\n",
    "                              alpha=0.2)\n",
    "        \n",
    "        ax_en.set_xlabel('Proportion of Class 1 (Triggering)', fontsize=12)\n",
    "        ax_en.set_ylabel(f'{metric.upper()} Score', fontsize=12)\n",
    "        ax_en.set_title(f'English - Sample Size {sample_size} - {metric.upper()}', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "        ax_en.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        ax_en.grid(True, alpha=0.3)\n",
    "        ax_en.set_ylim([0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_sample_size_comparison(results_df, metric='f1', distribution=0.5, save_path='sample_size_comparison.png'):\n",
    "    #compares how sample size affects metrics at a specific distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    data = results_df[np.isclose(results_df['distribution'], distribution, atol=0.01)]\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    arabic_data = data[data['language'] == 'arabic']\n",
    "    \n",
    "    models = arabic_data['model'].unique()\n",
    "    x = np.arange(len(SAMPLE_SIZES))\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = arabic_data[arabic_data['model'] == model].sort_values('sample_size')\n",
    "        positions = x + (i - len(models)/2) * width\n",
    "        ax1.bar(positions, model_data[f'{metric}_mean'], width, \n",
    "               label=model, alpha=0.8)\n",
    "        ax1.errorbar(positions, model_data[f'{metric}_mean'],\n",
    "                    yerr=[model_data[f'{metric}_mean'] - model_data[f'{metric}_ci_lower'],\n",
    "                          model_data[f'{metric}_ci_upper'] - model_data[f'{metric}_mean']],\n",
    "                    fmt='none', ecolor='black', capsize=3, alpha=0.6)\n",
    "    \n",
    "    ax1.set_xlabel('Sample Size', fontsize=12)\n",
    "    ax1.set_ylabel(f'{metric.upper()} Score', fontsize=12)\n",
    "    ax1.set_title(f'Arabic - {metric.upper()} by Sample Size (at {distribution:.0%} Class 1)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(SAMPLE_SIZES)\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_ylim([0, 1.05])\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    english_data = data[data['language'] == 'english']\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = english_data[english_data['model'] == model].sort_values('sample_size')\n",
    "        positions = x + (i - len(models)/2) * width\n",
    "        ax2.bar(positions, model_data[f'{metric}_mean'], width, \n",
    "               label=model, alpha=0.8)\n",
    "        ax2.errorbar(positions, model_data[f'{metric}_mean'],\n",
    "                    yerr=[model_data[f'{metric}_mean'] - model_data[f'{metric}_ci_lower'],\n",
    "                          model_data[f'{metric}_ci_upper'] - model_data[f'{metric}_mean']],\n",
    "                    fmt='none', ecolor='black', capsize=3, alpha=0.6)\n",
    "    \n",
    "    ax2.set_xlabel('Sample Size', fontsize=12)\n",
    "    ax2.set_ylabel(f'{metric.upper()} Score', fontsize=12)\n",
    "    ax2.set_title(f'English - {metric.upper()} by Sample Size (at {distribution:.0%} Class 1)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(SAMPLE_SIZES)\n",
    "    ax2.legend(fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim([0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def create_summary_table_with_ci(results_df, distribution=0.5, sample_size=500):\n",
    "    #summary table with confidence intervals\n",
    "    summary = results_df[(np.isclose(results_df['distribution'], distribution, atol=0.01)) & \n",
    "                         (results_df['sample_size'] == sample_size)].copy()\n",
    "    summary = summary.sort_values(['language', 'f1_mean'], ascending=[True, False])\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Summary at {distribution:.0%} Class 1 Distribution (Sample Size: {sample_size})\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"{'Model':<25} {'Lang':<7} {'Accuracy':<20} {'Precision':<20} {'Recall':<20} {'F1':<20}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    for _, row in summary.iterrows():\n",
    "        print(f\"{row['model']:<25} {row['language']:<7} \"\n",
    "              f\"{row['accuracy_mean']:.3f} [{row['accuracy_ci_lower']:.3f}-{row['accuracy_ci_upper']:.3f}]  \"\n",
    "              f\"{row['precision_mean']:.3f} [{row['precision_ci_lower']:.3f}-{row['precision_ci_upper']:.3f}]  \"\n",
    "              f\"{row['recall_mean']:.3f} [{row['recall_ci_lower']:.3f}-{row['recall_ci_upper']:.3f}]  \"\n",
    "              f\"{row['f1_mean']:.3f} [{row['f1_ci_lower']:.3f}-{row['f1_ci_upper']:.3f}]\")\n",
    "\n",
    "def analyze_sample_size_stability(results_df):\n",
    "    #checks how stable metrics are across different sample sizes\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Sample Size Stability Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"Shows variability in F1 score across sample sizes\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    for language in ['arabic', 'english']:\n",
    "        print(f\"\\n{language.upper()}:\")\n",
    "        for model in results_df['model'].unique():\n",
    "            model_data = results_df[(results_df['model'] == model) & \n",
    "                                   (results_df['language'] == language)]\n",
    "            \n",
    "            dist_50 = model_data[np.isclose(model_data['distribution'], 0.5, atol=0.01)]\n",
    "            \n",
    "            if len(dist_50) > 1:\n",
    "                f1_range = dist_50['f1_mean'].max() - dist_50['f1_mean'].min()\n",
    "                avg_ci_width = (dist_50['f1_ci_upper'] - dist_50['f1_ci_lower']).mean()\n",
    "                \n",
    "                print(f\"  {model:<25} F1 Range: {f1_range:.4f}, Avg CI Width: {avg_ci_width:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nStarting comprehensive evaluation...\")\n",
    "    print(f\"Target distributions: {len(TARGET_DISTRIBUTIONS)} from {TARGET_DISTRIBUTIONS[0]:.0%} to {TARGET_DISTRIBUTIONS[-1]:.0%}\")\n",
    "    print(f\"Sample sizes: {SAMPLE_SIZES}\")\n",
    "    print(f\"Iterations per distribution: {N_ITERATIONS}\")\n",
    "    print(f\"Confidence level: {CONFIDENCE_LEVEL*100:.0f}%\\n\")\n",
    "    \n",
    "    print(\"Verifying CSV files...\")\n",
    "    base_dir = os.getcwd()\n",
    "    all_files_exist = True\n",
    "    \n",
    "    for model_name, config in MODEL_CONFIGS.items():\n",
    "        for language in ['arabic', 'english']:\n",
    "            filepath = os.path.join(base_dir, config[language])\n",
    "            if not os.path.exists(filepath):\n",
    "                print(f\"  ✗ NOT FOUND: {filepath}\")\n",
    "                all_files_exist = False\n",
    "            else:\n",
    "                print(f\"  ✓ Found: {config[language]}\")\n",
    "    \n",
    "    if not all_files_exist:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ERROR: Some CSV files are missing.\")\n",
    "        print(\"=\"*80)\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All files found! Starting evaluation...\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    results, pred_dist_df = run_full_evaluation()\n",
    "    \n",
    "    output_path = os.path.join(base_dir, 'distribution_analysis_results_full.csv')\n",
    "    results.to_csv(output_path, index=False)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Results saved to '{output_path}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    plot_prediction_distributions(pred_dist_df, \n",
    "                                    save_path=os.path.join(base_dir, 'prediction_distributions.png'))\n",
    "\n",
    "    print_prediction_distribution_summary(pred_dist_df)\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        save_path = os.path.join(base_dir, f'{metric}_by_distribution_all_sizes.png')\n",
    "        plot_results_by_sample_size(results, metric=metric, save_path=save_path)\n",
    "\n",
    "    for sample_size in SAMPLE_SIZES:\n",
    "        save_path = os.path.join(base_dir, f'sample_size_comparison_{sample_size}.png')\n",
    "        plot_sample_size_comparison(results, metric='f1', distribution=0.5, save_path=save_path)\n",
    "\n",
    "    create_summary_table_with_ci(results, distribution=0.5, sample_size=500)\n",
    "\n",
    "    analyze_sample_size_stability(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    print(f\"  - distribution_analysis_results_full.csv\")\n",
    "    print(f\"  - model_prediction_distributions.csv\")\n",
    "    print(f\"  - prediction_distributions.png\")\n",
    "    print(f\"  - accuracy_by_distribution_all_sizes.png\")\n",
    "    print(f\"  - precision_by_distribution_all_sizes.png\")\n",
    "    print(f\"  - recall_by_distribution_all_sizes.png\")\n",
    "    print(f\"  - f1_by_distribution_all_sizes.png\")\n",
    "    for sample_size in SAMPLE_SIZES:\n",
    "        print(f\"  - sample_size_comparison_{sample_size}.png\")\n",
    "    print(f\"  - sampled_indices_[timestamp].json\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtais_demo_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
