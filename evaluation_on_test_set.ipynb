{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317f5731",
   "metadata": {},
   "source": [
    "# Evaluating xlmr base model and fine tuned models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259001ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA RTX 2000 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True\n",
    "print(torch.cuda.get_device_name(0))  # Should show your gpu, mine is NVIDIA RTX 2000 Ada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8718d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ba608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "\n",
    "MODELS = {\n",
    "    \"XLM-R-Multilingual\": \"anonym-author/xlmr-english-downsampled\",\n",
    "    \"XLM-R-Arabic\": \"anonym-author/xlmr-arabic-downsampled\",\n",
    "    \"XLM-R-English\": \"anonym-author/xlmr-english-downsampled\"\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Memory optimization settings\n",
    "BATCH_SIZE = 32 if torch.cuda.is_available() else 4  # Larger batches for RTX 2000\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd39083",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "parent = os.path.join(path, os.pardir)\n",
    "memo_dataset_path = os.path.join(parent, \"Data/Memo_Dataset.csv\")\n",
    "df = pd.read_csv(memo_dataset_path)\n",
    "\n",
    "df = df[['Question', 'Question_eng', 'Trigger']]\n",
    "\n",
    "print(\"Dataset unbalanced shape:\", df.shape)\n",
    "print(df['Trigger'].value_counts())\n",
    "\n",
    "# Balance dataset\n",
    "min_count = df['Trigger'].value_counts().min()\n",
    "df_balanced = (\n",
    "    df.groupby('Trigger', group_keys=False)\n",
    "    .apply(lambda x: x.sample(min_count, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nBalanced dataset:\", len(df_balanced))\n",
    "print(df_balanced['Trigger'].value_counts())\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced, test_size=0.2, random_state=42, stratify=df_balanced['Trigger']\n",
    ")\n",
    "\n",
    "print(f'\\nTraining samples: {len(train_df)}, Test samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881b1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, tokenizer, texts, batch_size=None):\n",
    "    \"\"\"Make predictions on a batch of texts\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = BATCH_SIZE\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            if (i + batch_size) % 80 == 0:\n",
    "                print(f\"  Processed {min(i + batch_size, len(texts))}/{len(texts)} samples...\")\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_probs)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, probs):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Confidence (max probability)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    correct_mask = (y_true == y_pred)\n",
    "    \n",
    "    metrics['avg_confidence_correct'] = confidences[correct_mask].mean() if correct_mask.sum() > 0 else 0\n",
    "    metrics['avg_confidence_incorrect'] = confidences[~correct_mask].mean() if (~correct_mask).sum() > 0 else 0\n",
    "    metrics['confusion_matrix'] = cm.tolist()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_predictions(df, predictions, probs, model_name, language, output_dir):\n",
    "    \"\"\"Save detailed predictions to CSV\"\"\"\n",
    "    results_df = df.copy()\n",
    "    results_df['prediction'] = predictions\n",
    "    results_df['prob_0'] = probs[:, 0]\n",
    "    results_df['prob_1'] = probs[:, 1]\n",
    "    results_df['confidence'] = np.max(probs, axis=1)\n",
    "    results_df['correct'] = results_df['Trigger'] == results_df['prediction']\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{model_name.replace('/', '_')}_{language}_{timestamp}.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    results_df.to_csv(filepath, index=False)\n",
    "    print(f\"  Saved predictions to: {filepath}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def print_evaluation_results(model_name, language, metrics):\n",
    "    \"\"\"Print formatted evaluation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Model: {model_name} | Language: {language}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"                Predicted 0  Predicted 1\")\n",
    "    print(f\"Actual 0        {cm[0][0]:>11}  {cm[0][1]:>11}\")\n",
    "    print(f\"Actual 1        {cm[1][0]:>11}  {cm[1][1]:>11}\")\n",
    "    \n",
    "    print(f\"\\nAvg Confidence (Correct):   {metrics['avg_confidence_correct']:.4f}\")\n",
    "    print(f\"Avg Confidence (Incorrect): {metrics['avg_confidence_incorrect']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main eval loop\n",
    "\n",
    "#output directory\n",
    "output_dir = os.path.join(path, \"xlmr_roberta_evaluation_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "for model_name, model_path in MODELS.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        # Evaluate on Arabic text\n",
    "        print(f\"\\nEvaluating on Arabic text...\")\n",
    "        arabic_texts = test_df['Question'].tolist()\n",
    "        arabic_preds, arabic_probs = predict_batch(model, tokenizer, arabic_texts)\n",
    "        arabic_metrics = calculate_metrics(test_df['Trigger'].values, arabic_preds, arabic_probs)\n",
    "        \n",
    "        # Save predictions\n",
    "        arabic_results_df = save_predictions(\n",
    "            test_df, arabic_preds, arabic_probs, model_name, \"Arabic\", output_dir\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print_evaluation_results(model_name, \"Arabic\", arabic_metrics)\n",
    "        \n",
    "        # Store for summary table\n",
    "        all_results.append({\n",
    "            'Model': model_name,\n",
    "            'Language': 'Arabic',\n",
    "            'Accuracy': arabic_metrics['accuracy'],\n",
    "            'Precision': arabic_metrics['precision'],\n",
    "            'Recall': arabic_metrics['recall'],\n",
    "            'F1': arabic_metrics['f1']\n",
    "        })\n",
    "        \n",
    "        # Evaluate on English text\n",
    "        print(f\"\\nEvaluating on English text...\")\n",
    "        english_texts = test_df['Question_eng'].tolist()\n",
    "        english_preds, english_probs = predict_batch(model, tokenizer, english_texts)\n",
    "        english_metrics = calculate_metrics(test_df['Trigger'].values, english_preds, english_probs)\n",
    "        \n",
    "        # Save predictions\n",
    "        english_results_df = save_predictions(\n",
    "            test_df, english_preds, english_probs, model_name, \"English\", output_dir\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print_evaluation_results(model_name, \"English\", english_metrics)\n",
    "        \n",
    "        # Store for summary table\n",
    "        all_results.append({\n",
    "            'Model': model_name,\n",
    "            'Language': 'English',\n",
    "            'Accuracy': english_metrics['accuracy'],\n",
    "            'Precision': english_metrics['precision'],\n",
    "            'Recall': english_metrics['recall'],\n",
    "            'F1': english_metrics['f1']\n",
    "        })\n",
    "        \n",
    "        # Clean up memory\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "summary_csv_path = os.path.join(output_dir, \"xlmr_roberta_evaluation_summary.csv\")\n",
    "results_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\nSummary table saved to: {summary_csv_path}\")\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec7704",
   "metadata": {},
   "source": [
    "# Generate Latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00979816",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = results_df.pivot(index='Model', columns='Language', values=['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "# Create LaTeX string\n",
    "latex_str = pivot_df.to_latex(\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Model Evaluation Results on Arabic Trigger Classification\",\n",
    "    label=\"tab:model_comparison\"\n",
    ")\n",
    "\n",
    "# Save LaTeX\n",
    "latex_path = os.path.join(output_dir, \"evaluation_summary.tex\")\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_str)\n",
    "print(f\"\\nLaTeX table saved to: {latex_path}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LATEX TABLE PREVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(latex_str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483b8d7",
   "metadata": {},
   "source": [
    "# Zero Shot Evals for Baseline XLMR Roberta model fine tuned on NLI tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfffb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'CUDA' if DEVICE == 0 else 'CPU'}\")\n",
    "\n",
    "# Multiple label formulations to test\n",
    "LABEL_FORMULATIONS = [\n",
    "    [\"concerning\", \"normal\"],\n",
    "    [\"urgent\", \"non-urgent\"],\n",
    "    [\"trigger\", \"non-trigger\"],\n",
    "    [\"distressed\", \"stable\"],\n",
    "    [\"needs support\", \"doing well\"]\n",
    "]\n",
    "\n",
    "print(\"\\nTesting label formulations:\")\n",
    "for labels in LABEL_FORMULATIONS:\n",
    "    print(f\"  - {labels}\")\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "parent = os.path.join(path, os.pardir)\n",
    "memo_dataset_path = os.path.join(parent, \"Data/Memo_Dataset.csv\")\n",
    "df = pd.read_csv(memo_dataset_path)\n",
    "\n",
    "df = df[['Question', 'Question_eng', 'Trigger']]\n",
    "\n",
    "# Balance dataset\n",
    "min_count = df['Trigger'].value_counts().min()\n",
    "df_balanced = (\n",
    "    df.groupby('Trigger', group_keys=False)\n",
    "    .apply(lambda x: x.sample(min_count, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Split data (SAME SPLIT AS FINE-TUNED MODELS)\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced, test_size=0.2, random_state=42, stratify=df_balanced['Trigger']\n",
    ")\n",
    "\n",
    "print(f'\\nTest samples: {len(test_df)}')\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loading XLM-RoBERTa NLI model for zero-shot classification\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use a model fine-tuned on multilingual NLI for better zero-shot performance\n",
    "# This model is specifically trained for zero-shot classification\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"joeddav/xlm-roberta-large-xnli\",  # Fine-tuned on XNLI (multilingual NLI)\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"Using joeddav/xlm-roberta-large-xnli (fine-tuned on XNLI)\")\n",
    "print(\"This model is optimized for zero-shot classification tasks\")\n",
    "\n",
    "\n",
    "def predict_zero_shot(texts, candidate_labels):\n",
    "    \"\"\"Make zero-shot predictions on texts\"\"\"\n",
    "    predictions = []\n",
    "    probs_list = []\n",
    "    \n",
    "    # Determine which label represents \"trigger\" (class 1)\n",
    "    # Assume first label is the positive/concerning/trigger label\n",
    "    trigger_label = candidate_labels[0].lower()\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        result = classifier(text, candidate_labels)\n",
    "        \n",
    "        # Get prediction (0 or 1)\n",
    "        pred_label = result['labels'][0]  # Top prediction\n",
    "        \n",
    "        # Check if top prediction is the trigger label\n",
    "        if pred_label.lower() == trigger_label:\n",
    "            prediction = 1  # Trigger\n",
    "        else:\n",
    "            prediction = 0  # Non-trigger\n",
    "        \n",
    "        # Get probabilities in [prob_0, prob_1] format\n",
    "        if result['labels'][0].lower() == trigger_label:\n",
    "            prob_1 = result['scores'][0]\n",
    "            prob_0 = result['scores'][1]\n",
    "        else:\n",
    "            prob_0 = result['scores'][0]\n",
    "            prob_1 = result['scores'][1]\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        probs_list.append([prob_0, prob_1])\n",
    "        \n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(texts)} samples...\")\n",
    "    \n",
    "    return np.array(predictions), np.array(probs_list)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, probs):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Confidence (max probability)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    correct_mask = (y_true == y_pred)\n",
    "    \n",
    "    metrics['avg_confidence_correct'] = confidences[correct_mask].mean() if correct_mask.sum() > 0 else 0\n",
    "    metrics['avg_confidence_incorrect'] = confidences[~correct_mask].mean() if (~correct_mask).sum() > 0 else 0\n",
    "    metrics['confusion_matrix'] = cm.tolist()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_predictions(df, predictions, probs, label_formulation, language, output_dir):\n",
    "    \"\"\"Save detailed predictions to CSV\"\"\"\n",
    "    results_df = df.copy()\n",
    "    results_df['prediction'] = predictions\n",
    "    results_df['prob_0'] = probs[:, 0]\n",
    "    results_df['prob_1'] = probs[:, 1]\n",
    "    results_df['confidence'] = np.max(probs, axis=1)\n",
    "    results_df['correct'] = results_df['Trigger'] == results_df['prediction']\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    label_str = \"_\".join(label_formulation).replace(\" \", \"-\")\n",
    "    filename = f\"ZeroShot_{label_str}_{language}_{timestamp}.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    results_df.to_csv(filepath, index=False)\n",
    "    print(f\"  Saved predictions to: {filename}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def print_evaluation_results(label_formulation, language, metrics):\n",
    "    \"\"\"Print formatted evaluation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Labels: {label_formulation} | Language: {language}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"                Predicted 0  Predicted 1\")\n",
    "    print(f\"Actual 0        {cm[0][0]:>11}  {cm[0][1]:>11}\")\n",
    "    print(f\"Actual 1        {cm[1][0]:>11}  {cm[1][1]:>11}\")\n",
    "    \n",
    "    print(f\"\\nAvg Confidence (Correct):   {metrics['avg_confidence_correct']:.4f}\")\n",
    "    print(f\"Avg Confidence (Incorrect): {metrics['avg_confidence_incorrect']:.4f}\")\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join(path, \"zeroshot_baseline_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "for labels in LABEL_FORMULATIONS:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Testing label formulation: {labels}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    label_str = str(labels)\n",
    "    \n",
    "    # ========================================\n",
    "    # Evaluate on Arabic\n",
    "    # ========================================\n",
    "    print(f\"\\nEvaluating on Arabic text...\")\n",
    "    \n",
    "    arabic_texts = test_df['Question'].tolist()\n",
    "    arabic_preds, arabic_probs = predict_zero_shot(arabic_texts, labels)\n",
    "    arabic_metrics = calculate_metrics(test_df['Trigger'].values, arabic_preds, arabic_probs)\n",
    "    \n",
    "    # Save predictions\n",
    "    arabic_results_df = save_predictions(\n",
    "        test_df, arabic_preds, arabic_probs, labels, \"Arabic\", output_dir\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print_evaluation_results(labels, \"Arabic\", arabic_metrics)\n",
    "    \n",
    "    # Store for summary table\n",
    "    all_results.append({\n",
    "        'Label_Formulation': label_str,\n",
    "        'Language': 'Arabic',\n",
    "        'Accuracy': arabic_metrics['accuracy'],\n",
    "        'Precision': arabic_metrics['precision'],\n",
    "        'Recall': arabic_metrics['recall'],\n",
    "        'F1': arabic_metrics['f1']\n",
    "    })\n",
    "    \n",
    "    # ========================================\n",
    "    # Evaluate on English\n",
    "    # ========================================\n",
    "    print(f\"\\nEvaluating on English text...\")\n",
    "    \n",
    "    english_texts = test_df['Question_eng'].tolist()\n",
    "    english_preds, english_probs = predict_zero_shot(english_texts, labels)\n",
    "    english_metrics = calculate_metrics(test_df['Trigger'].values, english_preds, english_probs)\n",
    "    \n",
    "    # Save predictions\n",
    "    english_results_df = save_predictions(\n",
    "        test_df, english_preds, english_probs, labels, \"English\", output_dir\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print_evaluation_results(labels, \"English\", english_metrics)\n",
    "    \n",
    "    # Store for summary table\n",
    "    all_results.append({\n",
    "        'Label_Formulation': label_str,\n",
    "        'Language': 'English',\n",
    "        'Accuracy': english_metrics['accuracy'],\n",
    "        'Precision': english_metrics['precision'],\n",
    "        'Recall': english_metrics['recall'],\n",
    "        'F1': english_metrics['f1']\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING ZERO-SHOT COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save as CSV\n",
    "summary_csv_path = os.path.join(output_dir, \"zeroshot_comparison_summary.csv\")\n",
    "results_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\nSummary saved to: {summary_csv_path}\")\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ZERO-SHOT LABEL FORMULATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST PERFORMING FORMULATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best for Arabic\n",
    "best_arabic = results_df[results_df['Language'] == 'Arabic'].nlargest(1, 'F1')\n",
    "print(f\"\\nBest for Arabic:\")\n",
    "print(f\"  Labels: {best_arabic['Label_Formulation'].values[0]}\")\n",
    "print(f\"  F1 Score: {best_arabic['F1'].values[0]:.4f}\")\n",
    "\n",
    "# Best for English\n",
    "best_english = results_df[results_df['Language'] == 'English'].nlargest(1, 'F1')\n",
    "print(f\"\\nBest for English:\")\n",
    "print(f\"  Labels: {best_english['Label_Formulation'].values[0]}\")\n",
    "print(f\"  F1 Score: {best_english['F1'].values[0]:.4f}\")\n",
    "\n",
    "\n",
    "# LaTeX format\n",
    "pivot_df = results_df.pivot(\n",
    "    index='Label_Formulation', \n",
    "    columns='Language', \n",
    "    values=['Accuracy', 'Precision', 'Recall', 'F1']\n",
    ")\n",
    "\n",
    "# Create LaTeX string\n",
    "latex_str = pivot_df.to_latex(\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Zero-Shot Performance Across Different Label Formulations\",\n",
    "    label=\"tab:zeroshot_comparison\"\n",
    ")\n",
    "\n",
    "# Save LaTeX\n",
    "latex_path = os.path.join(output_dir, \"zeroshot_comparison.tex\")\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "print(f\"\\nLaTeX table saved to: {latex_path}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LATEX TABLE PREVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(latex_str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ZERO-SHOT EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ab519",
   "metadata": {},
   "source": [
    "# Zero Shot Evals for Baseline XLMR-Roberta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b57ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "934afa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing label formulations:\n",
      "  - ['concerning', 'normal']\n",
      "  - ['urgent', 'non-urgent']\n",
      "  - ['trigger', 'non-trigger']\n",
      "  - ['distressed', 'stable']\n",
      "  - ['needs support', 'doing well']\n"
     ]
    }
   ],
   "source": [
    "# Multiple label categories to test\n",
    "LABEL_FORMULATIONS = [\n",
    "    [\"concerning\", \"normal\"],\n",
    "    [\"urgent\", \"non-urgent\"],\n",
    "    [\"trigger\", \"non-trigger\"],\n",
    "    [\"distressed\", \"stable\"],\n",
    "    [\"needs support\", \"doing well\"]\n",
    "]\n",
    "\n",
    "print(\"\\nTesting label formulations:\")\n",
    "for labels in LABEL_FORMULATIONS:\n",
    "    print(f\"  - {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "parent = os.path.join(path, os.pardir)\n",
    "memo_dataset_path = os.path.join(parent, \"Data/Memo_Dataset.csv\")\n",
    "df = pd.read_csv(memo_dataset_path)\n",
    "\n",
    "df = df[['Question', 'Question_eng', 'Trigger']]\n",
    "\n",
    "# Balance dataset\n",
    "min_count = df['Trigger'].value_counts().min()\n",
    "df_balanced = (\n",
    "    df.groupby('Trigger', group_keys=False)\n",
    "    .apply(lambda x: x.sample(min_count, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Split data (SAME SPLIT AS FINE-TUNED MODELS)\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced, test_size=0.2, random_state=42, stratify=df_balanced['Trigger']\n",
    ")\n",
    "\n",
    "print(f'\\nTest samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f378a6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Loading XLM-RoBERTa base model for zero-shot baseline classification\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loading XLM-RoBERTa base model for zero-shot baseline classification\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"xlm-roberta-base\",\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5543eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_zero_shot(texts, candidate_labels):\n",
    "    \"\"\"Make zero-shot predictions on texts\"\"\"\n",
    "    predictions = []\n",
    "    probs_list = []\n",
    "    \n",
    "    # Determine which label represents \"trigger\" (class 1)\n",
    "    # Assume first label is the positive/concerning/trigger label\n",
    "    trigger_label = candidate_labels[0].lower()\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        result = classifier(text, candidate_labels)\n",
    "        \n",
    "        # Get prediction (0 or 1)\n",
    "        pred_label = result['labels'][0]  # Top prediction\n",
    "        \n",
    "        # Check if top prediction is the trigger label\n",
    "        if pred_label.lower() == trigger_label:\n",
    "            prediction = 1  # Trigger\n",
    "        else:\n",
    "            prediction = 0  # Non-trigger\n",
    "        \n",
    "        # Get probabilities in [prob_0, prob_1] format\n",
    "        if result['labels'][0].lower() == trigger_label:\n",
    "            prob_1 = result['scores'][0]\n",
    "            prob_0 = result['scores'][1]\n",
    "        else:\n",
    "            prob_0 = result['scores'][0]\n",
    "            prob_1 = result['scores'][1]\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        probs_list.append([prob_0, prob_1])\n",
    "        \n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(texts)} samples...\")\n",
    "    \n",
    "    return np.array(predictions), np.array(probs_list)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, probs):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Confidence (max probability)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    correct_mask = (y_true == y_pred)\n",
    "    \n",
    "    metrics['avg_confidence_correct'] = confidences[correct_mask].mean() if correct_mask.sum() > 0 else 0\n",
    "    metrics['avg_confidence_incorrect'] = confidences[~correct_mask].mean() if (~correct_mask).sum() > 0 else 0\n",
    "    metrics['confusion_matrix'] = cm.tolist()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_predictions(df, predictions, probs, label_formulation, language, output_dir):\n",
    "    \"\"\"Save detailed predictions to CSV\"\"\"\n",
    "    results_df = df.copy()\n",
    "    results_df['prediction'] = predictions\n",
    "    results_df['prob_0'] = probs[:, 0]\n",
    "    results_df['prob_1'] = probs[:, 1]\n",
    "    results_df['confidence'] = np.max(probs, axis=1)\n",
    "    results_df['correct'] = results_df['Trigger'] == results_df['prediction']\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    label_str = \"_\".join(label_formulation).replace(\" \", \"-\")\n",
    "    filename = f\"ZeroShot_base_{label_str}_{language}_{timestamp}.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    results_df.to_csv(filepath, index=False)\n",
    "    print(f\"  Saved predictions to: {filename}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def print_evaluation_results(label_formulation, language, metrics):\n",
    "    \"\"\"Print formatted evaluation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Labels: {label_formulation} | Language: {language}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"                Predicted 0  Predicted 1\")\n",
    "    print(f\"Actual 0        {cm[0][0]:>11}  {cm[0][1]:>11}\")\n",
    "    print(f\"Actual 1        {cm[1][0]:>11}  {cm[1][1]:>11}\")\n",
    "    \n",
    "    print(f\"\\nAvg Confidence (Correct):   {metrics['avg_confidence_correct']:.4f}\")\n",
    "    print(f\"Avg Confidence (Incorrect): {metrics['avg_confidence_incorrect']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = os.path.join(path, \"zeroshot_base_xlmr_roberta_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "for labels in LABEL_FORMULATIONS:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Testing label formulation: {labels}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    label_str = str(labels)\n",
    "    \n",
    "    # ========================================\n",
    "    # Evaluate on Arabic\n",
    "    # ========================================\n",
    "    print(f\"\\nEvaluating on Arabic text...\")\n",
    "    \n",
    "    arabic_texts = test_df['Question'].tolist()\n",
    "    arabic_preds, arabic_probs = predict_zero_shot(arabic_texts, labels)\n",
    "    arabic_metrics = calculate_metrics(test_df['Trigger'].values, arabic_preds, arabic_probs)\n",
    "    \n",
    "    # Save predictions\n",
    "    arabic_results_df = save_predictions(\n",
    "        test_df, arabic_preds, arabic_probs, labels, \"Arabic\", output_dir\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print_evaluation_results(labels, \"Arabic\", arabic_metrics)\n",
    "    \n",
    "    # Store for summary table\n",
    "    all_results.append({\n",
    "        'Label_Formulation': label_str,\n",
    "        'Language': 'Arabic',\n",
    "        'Accuracy': arabic_metrics['accuracy'],\n",
    "        'Precision': arabic_metrics['precision'],\n",
    "        'Recall': arabic_metrics['recall'],\n",
    "        'F1': arabic_metrics['f1']\n",
    "    })\n",
    "    \n",
    "    # ========================================\n",
    "    # Evaluate on English\n",
    "    # ========================================\n",
    "    print(f\"\\nEvaluating on English text...\")\n",
    "    \n",
    "    english_texts = test_df['Question_eng'].tolist()\n",
    "    english_preds, english_probs = predict_zero_shot(english_texts, labels)\n",
    "    english_metrics = calculate_metrics(test_df['Trigger'].values, english_preds, english_probs)\n",
    "    \n",
    "    # Save predictions\n",
    "    english_results_df = save_predictions(\n",
    "        test_df, english_preds, english_probs, labels, \"English\", output_dir\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print_evaluation_results(labels, \"English\", english_metrics)\n",
    "    \n",
    "    # Store for summary table\n",
    "    all_results.append({\n",
    "        'Label_Formulation': label_str,\n",
    "        'Language': 'English',\n",
    "        'Accuracy': english_metrics['accuracy'],\n",
    "        'Precision': english_metrics['precision'],\n",
    "        'Recall': english_metrics['recall'],\n",
    "        'F1': english_metrics['f1']\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING ZERO-SHOT COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save as CSV\n",
    "summary_csv_path = os.path.join(output_dir, \"zeroshot_base_comparison_summary.csv\")\n",
    "results_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\nSummary saved to: {summary_csv_path}\")\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ZERO-SHOT LABEL FORMULATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST PERFORMING FORMULATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best for Arabic\n",
    "best_arabic = results_df[results_df['Language'] == 'Arabic'].nlargest(1, 'F1')\n",
    "print(f\"\\nBest for Arabic:\")\n",
    "print(f\"  Labels: {best_arabic['Label_Formulation'].values[0]}\")\n",
    "print(f\"  F1 Score: {best_arabic['F1'].values[0]:.4f}\")\n",
    "\n",
    "# Best for English\n",
    "best_english = results_df[results_df['Language'] == 'English'].nlargest(1, 'F1')\n",
    "print(f\"\\nBest for English:\")\n",
    "print(f\"  Labels: {best_english['Label_Formulation'].values[0]}\")\n",
    "print(f\"  F1 Score: {best_english['F1'].values[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be6f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for better LaTeX format\n",
    "pivot_df = results_df.pivot(\n",
    "    index='Label_Formulation', \n",
    "    columns='Language', \n",
    "    values=['Accuracy', 'Precision', 'Recall', 'F1']\n",
    ")\n",
    "\n",
    "# Create LaTeX string\n",
    "latex_str = pivot_df.to_latex(\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Zero-Shot Performance Across Different Label Formulations\",\n",
    "    label=\"tab:zeroshot_base_comparison\"\n",
    ")\n",
    "\n",
    "# Save LaTeX\n",
    "latex_path = os.path.join(output_dir, \"zeroshot_base_comparison.tex\")\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "print(f\"\\nLaTeX table saved to: {latex_path}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LATEX TABLE PREVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(latex_str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ZERO-SHOT EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"All results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtais_demo_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
