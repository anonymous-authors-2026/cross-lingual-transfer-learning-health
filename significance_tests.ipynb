{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5dd5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21874df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"McNemar's Test: Fine-Tuned XLM-RoBERTa vs Baselines\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "path = os.getcwd()                    \n",
    "FINETUNED_DIR = os.path.join(path,\"xlmr_roberta_evaluation_results\")  # where fine-tuned model CSVs are\n",
    "BASELINE_BASE_DIR = os.path.join(path, \"zeroshot_base_xlmr_roberta_results\")  # base XLM-RoBERTa (no fine-tuning)\n",
    "BASELINE_NLI_DIR = os.path.join(path, \"zeroshot_baseline_xlm_roberta_nli_results\")  # XLM-RoBERTa fine-tuned on NLI\n",
    "\n",
    "print(FINETUNED_DIR)\n",
    "print(BASELINE_BASE_DIR)\n",
    "print(BASELINE_NLI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2401b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions_from_csv(filepath):\n",
    "    \"\"\"\n",
    "    Load predictions from a CSV file.\n",
    "    Returns dictionary with y_true, y_pred arrays (excluding parse failures)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        #filter out parse failures if they exist (prediction == 3)\n",
    "        if 'prediction' in df.columns:\n",
    "            valid_mask = df['prediction'] != 3\n",
    "            df_filtered = df[valid_mask]\n",
    "        else:\n",
    "            df_filtered = df\n",
    "        \n",
    "        return {\n",
    "            'y_true': df_filtered['Trigger'].values,\n",
    "            'y_pred': df_filtered['prediction'].values,\n",
    "            'correct': df_filtered['correct'].values,\n",
    "            'filepath': filepath,\n",
    "            'n_samples': len(df_filtered)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_csv_file(directory, model_name, language):\n",
    "    \"\"\"Find the most recent CSV file matching model and language\"\"\"\n",
    "    pattern = os.path.join(directory, f\"{model_name}_{language}_*.csv\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"Warning: No files found matching {pattern}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract timestamp from filename and use it to find most recent\n",
    "    # Timestamp format: YYYYMMDD_HHMMSS\n",
    "    def get_timestamp_from_filename(filepath):\n",
    "        basename = os.path.basename(filepath)\n",
    "        # Extract the timestamp part (last part before .csv)\n",
    "        # Format: ModelName_Language_YYYYMMDD_HHMMSS.csv\n",
    "        parts = basename.replace('.csv', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            # Last two parts should be date and time\n",
    "            try:\n",
    "                timestamp_str = parts[-2] + parts[-1]  # Concatenate YYYYMMDD and HHMMSS\n",
    "                return timestamp_str\n",
    "            except:\n",
    "                return '0'\n",
    "        return '0'\n",
    "    \n",
    "    # Get most recent file based on timestamp in filename\n",
    "    most_recent = max(files, key=get_timestamp_from_filename)\n",
    "    return most_recent\n",
    "\n",
    "def perform_mcnemar_test(model1_pred, model2_pred, y_true, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test comparing two models\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    #ensure same samples\n",
    "    assert len(model1_pred) == len(model2_pred) == len(y_true), \\\n",
    "        \"Prediction arrays must have same length\"\n",
    "    \n",
    "    correct1 = (model1_pred == y_true)\n",
    "    correct2 = (model2_pred == y_true)\n",
    "    \n",
    "    #contingency table\n",
    "    both_correct = np.sum(correct1 & correct2)\n",
    "    only_1_correct = np.sum(correct1 & ~correct2)\n",
    "    only_2_correct = np.sum(~correct1 & correct2)\n",
    "    both_wrong = np.sum(~correct1 & ~correct2)\n",
    "    \n",
    "    # McNemar's test contingency table format\n",
    "    contingency_table = np.array([[both_correct, only_1_correct],\n",
    "                                   [only_2_correct, both_wrong]])\n",
    "    \n",
    "    #performs test\n",
    "    result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "    \n",
    "    acc1 = np.mean(correct1)\n",
    "    acc2 = np.mean(correct2)\n",
    "    \n",
    "    return {\n",
    "        'model1': model1_name,\n",
    "        'model2': model2_name,\n",
    "        'statistic': result.statistic,\n",
    "        'p_value': result.pvalue,\n",
    "        'both_correct': both_correct,\n",
    "        'only_model1_correct': only_1_correct,\n",
    "        'only_model2_correct': only_2_correct,\n",
    "        'both_wrong': both_wrong,\n",
    "        'model1_accuracy': acc1,\n",
    "        'model2_accuracy': acc2,\n",
    "        'significant': result.pvalue < 0.05,\n",
    "        'n_samples': len(y_true)\n",
    "    }\n",
    "\n",
    "\n",
    "def print_test_result(result, language):\n",
    "    \"\"\"Pretty print McNemar's test results\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Language: {language}\")\n",
    "    print(f\"Comparing: {result['model1']} vs {result['model2']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sample size: {result['n_samples']}\")\n",
    "    print(f\"\\nAccuracies:\")\n",
    "    print(f\"  {result['model1']}: {result['model1_accuracy']:.4f}\")\n",
    "    print(f\"  {result['model2']}: {result['model2_accuracy']:.4f}\")\n",
    "    print(f\"  Difference: {result['model1_accuracy'] - result['model2_accuracy']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nContingency Table:\")\n",
    "    print(f\"  Both correct:              {result['both_correct']}\")\n",
    "    print(f\"  Only {result['model1'][:20]:20} correct: {result['only_model1_correct']}\")\n",
    "    print(f\"  Only {result['model2'][:20]:20} correct: {result['only_model2_correct']}\")\n",
    "    print(f\"  Both wrong:                {result['both_wrong']}\")\n",
    "    \n",
    "    print(f\"\\nMcNemar's Test:\")\n",
    "    print(f\"  Chi-squared statistic: {result['statistic']:.4f}\")\n",
    "    print(f\"  P-value: {result['p_value']:.4f}\")\n",
    "    print(f\"  Significance level: α = 0.05\")\n",
    "    \n",
    "    if result['significant']:\n",
    "        if result['only_model1_correct'] > result['only_model2_correct']:\n",
    "            print(f\"\\nRESULT: {result['model1']} is SIGNIFICANTLY BETTER (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"\\nRESULT: {result['model2']} is SIGNIFICANTLY BETTER (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"\\nRESULT: No significant difference (p ≥ 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3402697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models to compare to baseline\n",
    "FINETUNED_MODELS = [\n",
    "    \"XLM-R-Multilingual\",\n",
    "    \"XLM-R-Arabic\", \n",
    "    \"XLM-R-English\"\n",
    "]\n",
    "\n",
    "# Dictionary of baselines: {display_name: (directory, file_prefix)}\n",
    "BASELINES = {\n",
    "    \"XLM-R-Base (Zero-Shot)\": (BASELINE_BASE_DIR, \"ZeroShot_base_trigger_non-trigger\"),\n",
    "    \"XLM-R-NLI (Zero-Shot)\": (BASELINE_NLI_DIR, \"ZeroShot_trigger_non-trigger\")\n",
    "}\n",
    "\n",
    "LANGUAGES = [\"Arabic\", \"English\"]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for language in LANGUAGES:\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# TESTING: {language.upper()}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Load all baseline models for this language\n",
    "    baseline_data_dict = {}\n",
    "    \n",
    "    for baseline_display_name, (baseline_dir, baseline_prefix) in BASELINES.items():\n",
    "        baseline_file = find_csv_file(baseline_dir, baseline_prefix, language)\n",
    "        \n",
    "        if baseline_file is None:\n",
    "            print(f\"Warning: Could not find {baseline_display_name} for {language}\")\n",
    "            continue\n",
    "        \n",
    "        baseline_data = load_predictions_from_csv(baseline_file)\n",
    "        if baseline_data is None:\n",
    "            print(f\"Warning: Could not load {baseline_display_name} for {language}\")\n",
    "            continue\n",
    "        \n",
    "        baseline_data_dict[baseline_display_name] = baseline_data\n",
    "        print(f\"\\nLoaded baseline: {baseline_display_name}\")\n",
    "        print(f\"  File: {baseline_file}\")\n",
    "        print(f\"  Samples: {baseline_data['n_samples']}\")\n",
    "    \n",
    "    if not baseline_data_dict:\n",
    "        print(f\"Skipping {language}: No baseline files found\")\n",
    "        continue\n",
    "    \n",
    "    # Compare each fine-tuned model against each baseline\n",
    "    for ft_model in FINETUNED_MODELS:\n",
    "        # Load fine-tuned model predictions\n",
    "        ft_file = find_csv_file(FINETUNED_DIR, ft_model, language)\n",
    "        \n",
    "        if ft_file is None:\n",
    "            print(f\"Skipping {ft_model}: File not found\")\n",
    "            continue\n",
    "        \n",
    "        ft_data = load_predictions_from_csv(ft_file)\n",
    "        if ft_data is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'*'*70}\")\n",
    "        print(f\"Fine-tuned model: {ft_model}\")\n",
    "        print(f\"  File: {ft_file}\")\n",
    "        print(f\"  Samples: {ft_data['n_samples']}\")\n",
    "        print(f\"{'*'*70}\")\n",
    "        \n",
    "        # Compare against each baseline\n",
    "        for baseline_name, baseline_data in baseline_data_dict.items():\n",
    "            \n",
    "            if ft_data['n_samples'] != baseline_data['n_samples']:\n",
    "                print(f\"Warning: Sample size mismatch!\")\n",
    "                print(f\"  Fine-tuned: {ft_data['n_samples']}, {baseline_name}: {baseline_data['n_samples']}\")\n",
    "                # Take minimum to be safe\n",
    "                min_samples = min(ft_data['n_samples'], baseline_data['n_samples'])\n",
    "                ft_pred = ft_data['y_pred'][:min_samples]\n",
    "                baseline_pred = baseline_data['y_pred'][:min_samples]\n",
    "                y_true = ft_data['y_true'][:min_samples]\n",
    "            else:\n",
    "                ft_pred = ft_data['y_pred']\n",
    "                baseline_pred = baseline_data['y_pred']\n",
    "                y_true = ft_data['y_true']\n",
    "            \n",
    "            # McNemar's test\n",
    "            result = perform_mcnemar_test(\n",
    "                ft_pred,\n",
    "                baseline_pred,\n",
    "                y_true,\n",
    "                ft_model,\n",
    "                baseline_name\n",
    "            )\n",
    "            \n",
    "            result['language'] = language\n",
    "            print_test_result(result, language)\n",
    "            \n",
    "            all_results.append({\n",
    "                'Fine-tuned Model': ft_model,\n",
    "                'Baseline': baseline_name,\n",
    "                'Language': language,\n",
    "                'FT Accuracy': result['model1_accuracy'],\n",
    "                'Baseline Accuracy': result['model2_accuracy'],\n",
    "                'Improvement': result['model1_accuracy'] - result['model2_accuracy'],\n",
    "                'P-value': result['p_value'],\n",
    "                'Significant (α=0.05)': 'Yes' if result['significant'] else '❌ No',\n",
    "                'Chi-squared': result['statistic']\n",
    "            })\n",
    "\n",
    "# Summary Table\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: Fine-tuned XLM-RoBERTa vs All Baselines\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_df = pd.DataFrame(all_results)\n",
    "\n",
    "if len(summary_df) > 0:\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "    output_path = os.getcwd()\n",
    "    \n",
    "    csv_path = os.path.join(output_path, \"mcnemar_test_results_all_baselines.csv\")\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nResults saved to: {csv_path}\")\n",
    "    \n",
    "    # Create a copy for LaTeX with formatted p-values\n",
    "    latex_df = summary_df[['Fine-tuned Model', 'Baseline', 'Language', 'FT Accuracy', \n",
    "                           'Baseline Accuracy', 'Improvement', 'P-value', 'Significant (α=0.05)']].copy()\n",
    "    \n",
    "    # Format p-values: show \"<0.01\" if p < 0.01, otherwise show actual value\n",
    "    latex_df['P-value'] = latex_df['P-value'].apply(\n",
    "        lambda x: '$<0.01$' if x < 0.01 else f'{x:.4f}'\n",
    "    )\n",
    "    \n",
    "    latex_str = latex_df.to_latex(\n",
    "        index=False,\n",
    "        caption=\"McNemar's Test: Fine-tuned XLM-RoBERTa vs All Baselines (α=0.05)\",\n",
    "        label=\"tab:mcnemar_all_baselines\",\n",
    "        escape=False\n",
    "    )\n",
    "    \n",
    "    latex_path = os.path.join(output_path, \"mcnemar_test_results_all_baselines.tex\")\n",
    "    with open(latex_path, 'w') as f:\n",
    "        f.write(latex_str)\n",
    "    print(f\"LaTeX table saved to: {latex_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY FINDINGS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Significant improvements count\n",
    "    sig_improvements = summary_df[summary_df['Significant (α=0.05)'] == 'Yes']\n",
    "    print(f\"\\nSignificant improvements: {len(sig_improvements)} out of {len(summary_df)}\")\n",
    "    \n",
    "    if len(sig_improvements) > 0:\n",
    "        print(\"\\nModels with significant improvement over baselines:\")\n",
    "        for _, row in sig_improvements.iterrows():\n",
    "            print(f\"  • {row['Fine-tuned Model']} ({row['Language']}) vs {row['Baseline']}: \"\n",
    "                  f\"+{row['Improvement']:.4f} accuracy, p={row['P-value']:.4f}\")\n",
    "    \n",
    "    # Best performing model\n",
    "    best_model = summary_df.loc[summary_df['FT Accuracy'].idxmax()]\n",
    "    print(f\"\\nBest performing fine-tuned model:\")\n",
    "    print(f\"  {best_model['Fine-tuned Model']} ({best_model['Language']})\")\n",
    "    print(f\"  Accuracy: {best_model['FT Accuracy']:.4f}\")\n",
    "    \n",
    "    # Group by baseline to see performance against each\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PERFORMANCE BY BASELINE\")\n",
    "    print(\"=\"*70)\n",
    "    for baseline_name in summary_df['Baseline'].unique():\n",
    "        baseline_subset = summary_df[summary_df['Baseline'] == baseline_name]\n",
    "        avg_improvement = baseline_subset['Improvement'].mean()\n",
    "        print(f\"\\n{baseline_name}:\")\n",
    "        print(f\"  Average improvement: +{avg_improvement:.4f}\")\n",
    "        print(f\"  Significant improvements: {len(baseline_subset[baseline_subset['Significant (α=0.05)'] == 'Yes'])} out of {len(baseline_subset)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo results to display. Check file paths.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtais_demo_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
